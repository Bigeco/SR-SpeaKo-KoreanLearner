import { ArrowLeft, Plus } from 'lucide-react';
import React, { useEffect, useRef, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { AudioRecorder } from '../../components/common/AudioRecorder';
import RecordControls from '../../components/common/RecordControls';
import { NavBar } from '../../components/layout/NavBar';
import { ScoreDisplay } from './components/ScoreDisplay';
import { SproutScore } from './components/SproutScore';
import TranscriptionCard from './components/TranscriptionCard';
import { getRomanizationAlignments } from '../../utils/romanizer_api';
import { 
  transcribeAudioWithWav2Vec2, 
  checkWav2Vec2ServerHealth,
  validateAudioFile,
  validateAudioSize,
  transcribeAudioWithSubmit,
  downloadAudioForAnalysis,
  analyzeAudioBlob
} from '../../utils/wav2vec2_api';
import './styles/start-record.css';

// Web Speech API íƒ€ì… ì •ì˜
interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  abort(): void;
  onstart: ((this: SpeechRecognition, ev: Event) => any) | null;
  onend: ((this: SpeechRecognition, ev: Event) => any) | null;
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null;
  onerror: ((this: SpeechRecognition, ev: Event) => any) | null;
}

declare global {
  interface Window {
    SpeechRecognition: {
      new (): SpeechRecognition;
    };
    webkitSpeechRecognition: {
      new (): SpeechRecognition;
    };
  }
}

const StartRecordView: React.FC = () => {
  const navigate = useNavigate();
  
  // ìƒíƒœ ê´€ë¦¬
  const [recordingState, setRecordingState] = useState<'idle' | 'recording' | 'completed'>('idle');
  const [isPlaying, setIsPlaying] = useState(false);
  const [showHelp, setShowHelp] = useState(false);
  
  // ì „ì‚¬ ë° êµì • í…ìŠ¤íŠ¸
  const [transcribedText, setTranscribedText] = useState<string>(''); // Wav2Vec2 ê²°ê³¼ (ì¸ì‹ëœ ë¬¸ì¥)
  const [correctedText, setCorrectedText] = useState<string>('');     // Web Speech API ê²°ê³¼ (êµì •ëœ ë¬¸ì¥)
  const [interimText, setInterimText] = useState<string>('');         // Web Speech API ì¤‘ê°„ ê²°ê³¼
  
  // ëˆ„ì ëœ ìµœì¢… í…ìŠ¤íŠ¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ìƒíƒœ
  const [accumulatedWebSpeechText, setAccumulatedWebSpeechText] = useState<string>('');
  const accumulatedWebSpeechTextRef = useRef('');
  useEffect(() => { accumulatedWebSpeechTextRef.current = accumulatedWebSpeechText; }, [accumulatedWebSpeechText]);
  
  // ë°œìŒ ì •í™•ë„ - ë…¹ìŒ ì™„ë£Œ ì‹œì—ë§Œ ì„¤ì •
  const [accuracy, setAccuracy] = useState<number | null>(null);
  
  // í‹€ë¦° ìŒì†Œë“¤ (ì‹¤ì œë¡œëŠ” AI ë¶„ì„ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¬ ë°ì´í„°)
  const [incorrectPhonemes, setIncorrectPhonemes] = useState<string[]>([]);
  
  // Web Speech API ê´€ë ¨
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [isSupported, setIsSupported] = useState<boolean>(false);
  const [micPermission, setMicPermission] = useState<'prompt' | 'granted' | 'denied'>('prompt');
  
  // ì˜¤ë””ì˜¤ ìš”ì†Œ ì°¸ì¡°
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const [recordedAudioBlob, setRecordedAudioBlob] = useState<Blob | null>(null);
  
  // Wav2Vec2 ì„œë²„ ìƒíƒœ
  const [wav2vec2ServerReady, setWav2vec2ServerReady] = useState<boolean>(false);
  const [isProcessingWav2Vec2, setIsProcessingWav2Vec2] = useState<boolean>(false);

  // recordingStateë¥¼ refë¡œ ì¶”ì 
  const recordingStateRef = useRef(recordingState);
  useEffect(() => { recordingStateRef.current = recordingState; }, [recordingState]);
  
  // ë¡œë§ˆì ì •ë ¬ ê´€ë ¨
  const [romanizationAlignments, setRomanizationAlignments] = useState<{ wrong: string[], correct: string[] } | null>(null);
  
  // Web Speech API ì§€ì› í™•ì¸ ë° ê¶Œí•œ ìš”ì²­
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (SpeechRecognition) {
      setIsSupported(true);
      recognitionRef.current = new SpeechRecognition();
      setupSpeechRecognition();
      
      // ë§ˆì´í¬ ê¶Œí•œ ë¯¸ë¦¬ í™•ì¸
      checkMicrophonePermission();
    } else {
      setIsSupported(false);
      console.warn('Web Speech APIê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.');
    }
  
    // Wav2Vec2 ì„œë²„ ìƒíƒœ í™•ì¸
    checkWav2Vec2ServerHealth()
      .then(setWav2vec2ServerReady)
      .catch(() => setWav2vec2ServerReady(false));
  }, []);
  
  // ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
  const checkMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop()); // ìŠ¤íŠ¸ë¦¼ ì¦‰ì‹œ ì •ì§€
      setMicPermission('granted');
    } catch (error) {
      console.log('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      setMicPermission('denied');
    }
  };
  
  // Speech Recognition ì„¤ì •
  const setupSpeechRecognition = () => {
    if (!recognitionRef.current) return;
    
    const recognition = recognitionRef.current;
    
    // ê¸°ë³¸ ì„¤ì •
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR'; // í•œêµ­ì–´ ì„¤ì •
    
    // ìŒì„± ì¸ì‹ ì‹œì‘
    recognition.onstart = () => {
      console.log('ìŒì„± ì¸ì‹ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.');
    };
    
    // ìŒì„± ì¸ì‹ ê²°ê³¼
    recognition.onresult = (event: SpeechRecognitionEvent) => {
      // ë…¹ìŒì´ ì¤‘ì§€ëœ ìƒíƒœë©´ ê²°ê³¼ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
      if (recordingStateRef.current !== 'recording') {
        console.log('ë…¹ìŒ ì¤‘ì§€ ìƒíƒœì´ë¯€ë¡œ ìŒì„± ì¸ì‹ ê²°ê³¼ ë¬´ì‹œ');
        return;
      }
      
      let finalTranscript = '';
      let interimTranscript = '';
      
      console.log('Web Speech API ê²°ê³¼ ë°›ìŒ:', event.results.length);
      
      // ëª¨ë“  ê²°ê³¼ ì²˜ë¦¬
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        console.log(`ê²°ê³¼ ${i}: "${transcript}", isFinal: ${event.results[i].isFinal}`);
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }
      
      // ìµœì¢… ê²°ê³¼ê°€ ìˆì„ ë•Œ ëˆ„ì  (ë…¹ìŒ ì¤‘ì¼ ë•Œë§Œ)
      if (finalTranscript && recordingStateRef.current === 'recording') {
        console.log('Web Speech API ìµœì¢… í…ìŠ¤íŠ¸ (êµì •ëœ ë¬¸ì¥):', `"${finalTranscript}"`);
        
        // ê¸°ì¡´ ëˆ„ì  í…ìŠ¤íŠ¸ì— ìƒˆë¡œìš´ ìµœì¢… í…ìŠ¤íŠ¸ ì¶”ê°€
        setAccumulatedWebSpeechText(prev => {
          const newAccumulated = prev ? `${prev} ${finalTranscript}`.trim() : finalTranscript;
          console.log('ëˆ„ì ëœ Web Speech API í…ìŠ¤íŠ¸:', `"${newAccumulated}"`);
          return newAccumulated;
        });
      }
      
      // ì¤‘ê°„ ê²°ê³¼ ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„ í‘œì‹œìš©, ë…¹ìŒ ì¤‘ì¼ ë•Œë§Œ)
      if (interimTranscript && recordingStateRef.current === 'recording') {
        console.log('ì¤‘ê°„ ê²°ê³¼ ì—…ë°ì´íŠ¸:', `"${interimTranscript}"`);
        setInterimText(interimTranscript);
      }
    };
    
    // ìŒì„± ì¸ì‹ ì¢…ë£Œ
    recognition.onend = () => {
      console.log('ìŒì„± ì¸ì‹ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
      if (recordingStateRef.current === 'recording') {
        // ìë™ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘ (continuousë¥¼ ìœ„í•´)
        try {
          recognition.start();
        } catch (error) {
          console.log('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜:', error);
        }
      }
    };
    
    // ì˜¤ë¥˜ ì²˜ë¦¬
    recognition.onerror = (event: any) => {
      console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error);
      if (event.error === 'not-allowed') {
        alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
      }
    };
  };
  
  // í˜ì´ì§€ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
  const handleGoBack = () => navigate(-1);
  
  // êµ¬ê°• êµ¬ì¡° í˜ì´ì§€ë¡œ ì´ë™
  const handleOralStructureView = () => {
    navigate('/oral-structure', {
      state: {
        incorrectPhonemes: incorrectPhonemes.length > 0 ? incorrectPhonemes : ['ã„±', 'ã…“', 'ã„¹'] // ì˜ˆì‹œ ë°ì´í„°
      }
    });
  };
  
  // í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ (êµ¬ë‘ì  ì œê±° ë° ê³µë°± ì •ë¦¬)
  const normalizeText = (text: string): string => {
    return text
      .replace(/[.,!?;:]/g, '') // êµ¬ë‘ì  ì œê±°
      .replace(/\s+/g, ' ')     // ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
      .trim();                  // ì•ë’¤ ê³µë°± ì œê±° (toLowerCase ì œê±°)
  };
  
  // í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
  const preprocessText = (
    text: string,
    removeSpaces = true,
    removePunctuation = true
  ): string => {
    if (removePunctuation) {
      text = text.replace(/[^\w\sã„±-ã…ê°€-í£]/g, ''); // ë¬¸ì¥ë¶€í˜¸ ì œê±°
    }
    if (removeSpaces) {
      text = text.replace(/\s+/g, '');
    }
    return text;
  };

  // ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°
  const calculateLevenshtein = (
    u: string[],
    v: string[]
  ): { distance: number; substitutions: number; deletions: number; insertions: number } => {
    const prev: number[] = Array(v.length + 1)
      .fill(0)
      .map((_, i) => i);
    const prevOps: [number, number, number][] = Array(v.length + 1)
      .fill(0)
      .map((_, i) => [0, 0, i]);

    let curr: number[] = [];
    let currOps: [number, number, number][] = [];

    for (let x = 1; x <= u.length; x++) {
      curr = [x, ...Array(v.length).fill(0)];
      currOps = [[0, x, 0], ...Array(v.length).fill([0, 0, 0])];

      for (let y = 1; y <= v.length; y++) {
        const delCost = prev[y] + 1;
        const addCost = curr[y - 1] + 1;
        const subCost = prev[y - 1] + (u[x - 1] !== v[y - 1] ? 1 : 0);

        curr[y] = Math.min(subCost, delCost, addCost);

        if (curr[y] === subCost) {
          const [s, d, i] = prevOps[y - 1];
          currOps[y] = [s + (u[x - 1] !== v[y - 1] ? 1 : 0), d, i];
        } else if (curr[y] === delCost) {
          const [s, d, i] = prevOps[y];
          currOps[y] = [s, d + 1, i];
        } else {
          const [s, d, i] = currOps[y - 1];
          currOps[y] = [s, d, i + 1];
        }
      }

      for (let i = 0; i < curr.length; i++) {
        prev[i] = curr[i];
        prevOps[i] = currOps[i];
      }
    }

    const [substitutions, deletions, insertions] = currOps[v.length];
    return {
      distance: curr[v.length],
      substitutions,
      deletions,
      insertions
    };
  };

  // ì •í™•ë„(CRR) ê³„ì‚° í•¨ìˆ˜
  const calculateAccuracy = (
    original: string,
    corrected: string,
    removeSpaces = true,
    removePunctuation = true
  ): number => {
    console.log('=== ì •í™•ë„(CRR) ê³„ì‚° ì‹œì‘ ===');
    console.log('ì›ë³¸:', original);
    console.log('êµì •:', corrected);

    if (!original || !corrected || original.trim() === '' || corrected.trim() === '') {
      console.log('âŒ ì…ë ¥ì´ ë¹„ì–´ ìˆìŒ');
      return 0;
    }

    const ref = preprocessText(original, removeSpaces, removePunctuation);
    const hyp = preprocessText(corrected, removeSpaces, removePunctuation);

    if (!ref || !hyp) {
      console.log('âŒ ì •ê·œí™” í›„ ë¹„ì–´ ìˆìŒ');
      return 0;
    }

    const refChars = ref.split('');
    const hypChars = hyp.split('');

    const { substitutions, deletions, insertions } = calculateLevenshtein(hypChars, refChars);

    const hits = refChars.length - (substitutions + deletions);
    const total = substitutions + deletions + insertions + hits;

    const cer = total > 0 ? (substitutions + deletions + insertions) / total : 0;
    const crr = Math.max(0, Math.min(1 - cer, 1));

    const finalResult = Math.round(crr * 1000) / 10; // ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€ (%)
    console.log('ğŸ“Š ê²°ê³¼:', {
      substitutions,
      deletions,
      insertions,
      crr: `${finalResult}%`
    });

    return finalResult;
  };

  
  const processAudioWithWav2Vec2 = async (audioBlob: Blob) => {
    console.log('ğŸ¤ Wav2Vec2 ì²˜ë¦¬ ì‹œì‘:', { size: audioBlob.size, type: audioBlob.type });
    
    try {
      setIsProcessingWav2Vec2(true);
      
      // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ë¶„ì„
      await analyzeAudioBlob(audioBlob);
      
      // ë””ë²„ê¹…: íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ë¹„êµìš©)
      await downloadAudioForAnalysis(audioBlob, 'app-recording.wav');
      
      // íŒŒì¼ ê²€ì¦
      if (!validateAudioFile(audioBlob)) {
        throw new Error('ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤.');
      }
      
      if (!validateAudioSize(audioBlob, 10)) {
        throw new Error('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€ 10MB)');
      }
      
      // ì¼ë°˜ API ë°©ì‹ í…ŒìŠ¤íŠ¸
      console.log('ğŸ“± ì•± ë°©ì‹ (/transcribe) í…ŒìŠ¤íŠ¸ ì¤‘...');
      const result = await transcribeAudioWithWav2Vec2(audioBlob, 'recording.wav');
      
      // ì›¹ UI ë°©ì‹ë„ í…ŒìŠ¤íŠ¸ (ë¹„êµìš©)
      console.log('ğŸŒ ì›¹ UI ë°©ì‹ (/submit) í…ŒìŠ¤íŠ¸ ì¤‘...');
      try {
        const webResult = await transcribeAudioWithSubmit(audioBlob, 'recording.wav');
        console.log('ğŸ”„ ê²°ê³¼ ë¹„êµ:', {
          ì•±ê²°ê³¼: result.transcription,
          ì›¹ê²°ê³¼: webResult.transcription,
          ë™ì¼í•¨: result.transcription === webResult.transcription
        });
      } catch (webError) {
        console.warn('ì›¹ UI ë°©ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:', webError);
      }
      
      setTranscribedText(result.transcription);
      return result.transcription;
      
    } catch (error) {
      console.error('Wav2Vec2 ì²˜ë¦¬ ì˜¤ë¥˜:', error);
      
      // ê¸°ì¡´ fallback ë¡œì§...
      const fallbackText = accumulatedWebSpeechTextRef.current || interimText;
      if (fallbackText) {
        console.log('Wav2Vec2 ì‹¤íŒ¨, Web Speech API ê²°ê³¼ë¡œ fallback:', fallbackText);
        setTranscribedText(fallbackText);
        return fallbackText;
      } else {
        const testText = "ìˆ˜í•™ì„ ë°°ì˜¤ê³  ìˆì–´ìš”";
        console.log('ì™„ì „ ì‹¤íŒ¨, í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©:', testText);
        setTranscribedText(testText);
        return testText;
      }
    } finally {
      setIsProcessingWav2Vec2(false);
    }
  };

  // í‹€ë¦° ìŒì†Œ ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜ - ì‹¤ì œë¡œëŠ” AIê°€ ë¶„ì„)
  const analyzeIncorrectPhonemes = (original: string, corrected: string): string[] => {
    const incorrectPhonemes: string[] = [];
    
    // ê°„ë‹¨í•œ ë‹¨ì–´ë³„ ë¹„êµ
    const originalWords = original.split(' ');
    const correctedWords = corrected.split(' ');
    
    for (let i = 0; i < originalWords.length; i++) {
      if (originalWords[i] !== correctedWords[i]) {
        // ì˜ˆì‹œ: 'ë°°ì˜¤ê³ ' vs 'ë°°ìš°ê³ ' -> 'ã…—' vs 'ã…œ' ì°¨ì´
        if (originalWords[i]?.includes('ë°°ì˜¤ê³ ') && correctedWords[i]?.includes('ë°°ìš°ê³ ')) {
          incorrectPhonemes.push('ã…—', 'ã…œ');
        }
        // ì˜ˆì‹œ: 'ìˆì„œìš”' vs 'ìˆì–´ìš”' -> 'ã……' vs 'ã…‡' ì°¨ì´  
        if (originalWords[i]?.includes('ìˆì„œìš”') && correctedWords[i]?.includes('ìˆì–´ìš”')) {
          incorrectPhonemes.push('ã……', 'ã…‡');
        }
      }
    }
    
    // ì¤‘ë³µ ì œê±° ë° ê¸°ë³¸ê°’
    const uniquePhonemes = [...new Set(incorrectPhonemes)];
    return uniquePhonemes.length > 0 ? uniquePhonemes : ['ã„±', 'ã…“', 'ã„¹'];
  };
  
  // ë…¹ìŒ ì‹œì‘/ì¤‘ì§€ ì²˜ë¦¬
  const handleRecordingToggle = async (isRecording: boolean) => {
    if (!isSupported) {
      alert('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chrome, Safari, Edgeë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.');
      return;
    }
    
    if (isRecording) {
      // ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ëœ ê²½ìš° ë‹¤ì‹œ ìš”ì²­
      if (micPermission === 'denied') {
        await checkMicrophonePermission();
        if (micPermission === 'denied') {
          alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
          return;
        }
      }
      
      // ë…¹ìŒ ì‹œì‘ - ëª¨ë“  ìƒíƒœ ì´ˆê¸°í™”
      setRecordingState('recording');
      setTranscribedText('');
      setAccumulatedWebSpeechText('');
      setCorrectedText('');
      setInterimText('');
      setAccuracy(null);
      setIncorrectPhonemes([]);
      setRecordedAudioBlob(null);
      
      // Web Speech API ì‹œì‘
      try {
        recognitionRef.current?.start();
      } catch (error) {
        console.error('ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜:', error);
      }
    } else {
      // ë…¹ìŒ ì¤‘ì§€
      console.log('ë…¹ìŒ ì¤‘ì§€ ì‹œì‘ - ìƒíƒœ ë³€ê²½');
      setRecordingState('completed'); // ë¨¼ì € ìƒíƒœ ë³€ê²½í•˜ì—¬ ì¶”ê°€ onresult ì´ë²¤íŠ¸ ì°¨ë‹¨
      
      recognitionRef.current?.stop();
      
      // ë” ê¸´ ì§€ì—°ìœ¼ë¡œ Web Speech API ìµœì¢… ê²°ê³¼ ëŒ€ê¸°
      setTimeout(async () => {
        // Web Speech API ìµœì¢… ê²°ê³¼ë¥¼ êµì •ëœ ë¬¸ì¥ìœ¼ë¡œ ì„¤ì •
        const webSpeechResult = accumulatedWebSpeechTextRef.current || interimText;
        
        console.log('ë…¹ìŒ ì¤‘ì§€ í›„ Web Speech API ê²°ê³¼ (êµì •ëœ ë¬¸ì¥):', `"${webSpeechResult}"`);
        
        if (webSpeechResult) {
          setCorrectedText(webSpeechResult);
        }
        
        // Wav2Vec2ë¡œ ìµœì¢… ì¸ì‹ ì²˜ë¦¬ (ë…¹ìŒëœ ì˜¤ë””ì˜¤ ìˆëŠ” ê²½ìš°)
        if (recordedAudioBlob) {
          console.log('ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¡œ Wav2Vec2 ì²˜ë¦¬ ì‹œì‘');
          const wav2vecResult = await processAudioWithWav2Vec2(recordedAudioBlob);
          
          // ì •í™•ë„ ê³„ì‚° (Wav2Vec2 ê²°ê³¼ vs Web Speech API ê²°ê³¼)
          const finalCorrectedText = webSpeechResult || "ìˆ˜í•™ì„ ë°°ìš°ê³  ìˆì–´ìš”"; // fallback
          const finalAccuracy = calculateAccuracy(wav2vecResult, finalCorrectedText);
          
          setAccuracy(finalAccuracy);
          setIncorrectPhonemes(analyzeIncorrectPhonemes(wav2vecResult, finalCorrectedText));
        } else {
          console.warn('ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì—†ìŒ, Wav2Vec2 ì²˜ë¦¬ ìŠ¤í‚µ');
          
          // Wav2Vec2 ì²˜ë¦¬ ì—†ì´ Web Speech API ê²°ê³¼ë§Œìœ¼ë¡œ ì²˜ë¦¬
          if (webSpeechResult) {
            setTranscribedText(webSpeechResult); // ì„ì‹œë¡œ ë™ì¼í•˜ê²Œ ì„¤ì •
            setAccuracy(100); // Web Speech API ê²°ê³¼ê°€ ì •ë‹µì´ë¯€ë¡œ 100%
          }
        }
        
        setInterimText(''); // ì¤‘ê°„ í…ìŠ¤íŠ¸ ì œê±°
      }, 1000);
    }
  };
  
  // ì˜¤ë””ì˜¤ ì¬ìƒ í† ê¸€ (ì‹¤ì œë¡œëŠ” ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì¬ìƒ)
  const togglePlayback = () => {
    setIsPlaying(!isPlaying);
    
    // ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ 3ì´ˆ í›„ ì¬ìƒ ìƒíƒœ í•´ì œ
    if (!isPlaying) {
      setTimeout(() => setIsPlaying(false), 3000);
    }
  };
  
  // ì „ì‚¬ ê²°ê³¼ì—ì„œ êµì •ëœ ë¶€ë¶„ í•˜ì´ë¼ì´íŠ¸
  const renderHighlightedCorrections = () => {
    const originalText = transcribedText;
    const correctionText = correctedText;
    
    if (!originalText || !correctionText) return null;
    
    const transcribedWords = originalText.trim().split(/\s+/);
    const correctedWords = correctionText.trim().split(/\s+/);
    
    return (
      <div className="mt-3 pt-3 border-t border-green-100">
        <p className="text-sm text-gray-600">êµì •ëœ ë¶€ë¶„:</p>
        <div className="mt-1 text-sm">
          {transcribedWords.map((word, idx) => {
            const correctedWord = correctedWords[idx];
            const isChanged = word !== correctedWord;
            
            return (
              <span key={idx} className="inline-block mr-2">
                <span className={isChanged ? 'line-through text-red-500' : ''}>
                  {word}
                </span>
                {isChanged && correctedWord && (
                  <span className="inline-block ml-1 text-green-500">
                    â†’ {correctedWord}
                  </span>
                )}
              </span>
            );
          })}
        </div>
      </div>
    );
  };

  // í˜„ì¬ í‘œì‹œí•  í…ìŠ¤íŠ¸ (ë…¹ìŒ ì¤‘: ëˆ„ì ëœ ìµœì¢… í…ìŠ¤íŠ¸ + ì¤‘ê°„ í…ìŠ¤íŠ¸, ì™„ë£Œ: ìµœì¢… í…ìŠ¤íŠ¸)
  const getCurrentDisplayText = () => {
    if (recordingState === 'recording') {
      // ë…¹ìŒ ì¤‘ì¼ ë•Œ: ëˆ„ì ëœ ìµœì¢… í…ìŠ¤íŠ¸ + í˜„ì¬ ì¤‘ê°„ í…ìŠ¤íŠ¸
      const baseText = accumulatedWebSpeechText;
      const currentText = interimText;
      return baseText && currentText ? `${baseText} ${currentText}` : (baseText || currentText);
    }
    // ì™„ë£Œ ìƒíƒœì¼ ë•Œ: transcribedText ì‚¬ìš©
    return transcribedText;
  };

  // Add toggleHelp handler
  const toggleHelp = () => setShowHelp((prev) => !prev);

  // recordingStateê°€ completedê°€ ë˜ê³ , transcribedTextì™€ correctedTextê°€ ëª¨ë‘ ìˆì„ ë•Œ ë¡œë§ˆì ì •ë ¬ í˜¸ì¶œ
  useEffect(() => {
    if (
      recordingState === 'completed' &&
      transcribedText &&
      correctedText
    ) {
      // ë¹„ë™ê¸° í˜¸ì¶œ
      getRomanizationAlignments(transcribedText, correctedText)
        .then(setRomanizationAlignments)
        .catch(() => setRomanizationAlignments(null));
    } else {
      setRomanizationAlignments(null);
    }
  }, [recordingState, transcribedText, correctedText]);

  return (
    <div className="start-record-container">
      {/* í—¤ë” */}
      <div className="flex justify-between items-center p-6 border-b border-gray-100">
        <button 
          onClick={handleGoBack}
          className="p-1 rounded-full hover:bg-gray-100"
        >
          <ArrowLeft size={20} />
        </button>
        <div className="text-center font-medium">ë°œìŒ í™•ì¸</div>
        <div className="w-5"></div>
      </div>

      {/* Web Speech API ì§€ì› í™•ì¸ ì•Œë¦¼ */}
      {!isSupported && (
        <div className="bg-yellow-50 border-l-4 border-yellow-400 p-4 m-4">
          <div className="flex">
            <div className="ml-3">
              <p className="text-sm text-yellow-700">
                ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
                Chrome, Safari, Edge ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
              </p>
            </div>
          </div>
        </div>
      )}

      {/* ë©”ì¸ ì½˜í…ì¸  - í•˜ë‹¨ ìš”ì†Œë“¤ ê³µê°„ í™•ë³´ë¥¼ ìœ„í•œ íŒ¨ë”© ì¶”ê°€ */}
      <div className="flex-1 flex flex-col items-center px-6 py-4 overflow-auto pb-44">
        <div className="w-full max-w-md">
          {/* ìƒˆì‹¹ ìºë¦­í„° UI */}
          <div className="flex flex-col items-center mb-6 mt-9">
            <div className={`transition-all duration-500 
              ${recordingState === 'recording' ? 'opacity-60 grayscale' : 'opacity-100 grayscale-0'}`}>
              <SproutScore score={accuracy ?? 0} size={120} />
            </div>
          </div>  

          {/* ì•ˆë‚´ í…ìŠ¤íŠ¸ ë° ì •í™•ë„ í‘œì‹œ */}
          <div className="text-center mb-6">
            <h2 className="text-xl font-bold text-blue-600 mb-2">
              {recordingState === 'idle' && 'ë‹¹ì‹ ì˜ ë°œìŒì„ í™•ì¸í•´ ë³´ì„¸ìš”'}
              {recordingState === 'recording' && 'ë§í•˜ëŠ” ì¤‘...'}
              {recordingState === 'completed' && 'ì¸ì‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'}
            </h2>
            <p className="text-gray-600 mb-2">
              {recordingState === 'idle' && 'ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ììœ ë¡­ê²Œ ë§í•´ë³´ì„¸ìš”.'}
              {recordingState === 'recording' && 'ì‹¤ì‹œê°„ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤.'}
              {recordingState === 'completed' && 'ì¸ì‹ëœ ë¬¸ì¥ê³¼ êµì •ëœ ë¬¸ì¥ì„ í™•ì¸í•˜ì„¸ìš”.'}
            </p>
            {/* ì •í™•ë„ ìˆ˜ì¹˜ì™€ í”¼ë“œë°± ë©”ì‹œì§€ */}
            {recordingState === 'completed' && accuracy !== null && (
              <ScoreDisplay score={accuracy} />
            )}
          </div>

          {/* Wav2Vec2 ì²˜ë¦¬ ì¤‘ ì•Œë¦¼ */}
          {isProcessingWav2Vec2 && (
            <div className="bg-blue-50 border-l-4 border-blue-400 p-4 m-4">
              <div className="flex">
                <div className="ml-3">
                  <p className="text-sm text-blue-700">
                    ğŸ¤ Wav2Vec2ë¡œ ìŒì„±ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...
                  </p>
                </div>
              </div>
            </div>
          )}

          {/* ì „ì‚¬ ê²°ê³¼ ì¹´ë“œ */}
          <TranscriptionCard
            recordingState={recordingState}
            transcribedText={getCurrentDisplayText()}
            correctedText={correctedText}
            isPlaying={isPlaying}
            onPlayAudio={togglePlayback}
            renderHighlightedCorrections={renderHighlightedCorrections}
            wrongRomanizations={romanizationAlignments?.wrong}
            correctRomanizations={romanizationAlignments?.correct}
          />
          
          {/* ì¶”ê°€ ì•ˆë‚´ (idle ìƒíƒœì¼ ë•Œ) */}
          {recordingState === 'idle' && (
            <div className="bg-blue-50 rounded-lg p-4 text-center text-sm text-blue-700">
              <p>ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ë§í•´ë³´ì„¸ìš”. ë°œìŒì´ ìë™ìœ¼ë¡œ êµì •ë©ë‹ˆë‹¤.</p>
              <p className="mt-2">ì˜ˆì‹œ: "ì•ˆë…•í•˜ì„¸ìš”", "ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "í•œêµ­ì–´ ê³µë¶€ê°€ ì¬ë¯¸ìˆì–´ìš”" ë“±</p>
              {!isSupported && (
                <p className="mt-2 text-red-600">
                  âš ï¸ í˜„ì¬ ë¸Œë¼ìš°ì €ì—ì„œëŠ” ìŒì„± ì¸ì‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
                </p>
              )}
              {!wav2vec2ServerReady && (
                <p className="mt-2 text-orange-600">
                  âš ï¸ Wav2Vec2 ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ Web Speech APIë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
                </p>
              )}
            </div>
          )}
          
          {/* ë¶„ì„ëœ í‹€ë¦° ìŒì†Œ ë¯¸ë¦¬ë³´ê¸° (completed ìƒíƒœì¼ ë•Œë§Œ) */}
          {recordingState === 'completed' && incorrectPhonemes.length > 0 && (
            <div className="bg-orange-50 rounded-lg p-4 mt-4">
              <h3 className="text-sm font-semibold text-orange-700 mb-2">ê°œì„ ì´ í•„ìš”í•œ ë°œìŒ</h3>
              <div className="flex flex-wrap gap-2 mb-3">
                {incorrectPhonemes.map((phoneme, index) => (
                  <span 
                    key={index}
                    className="inline-block bg-orange-200 text-orange-800 px-2 py-1 rounded text-sm font-medium"
                  >
                    {phoneme}
                  </span>
                ))}
              </div>
              <p className="text-orange-600 text-xs">
                êµ¬ê°• êµ¬ì¡° í•™ìŠµì„ í†µí•´ ì •í™•í•œ ë°œìŒë²•ì„ ìµí˜€ë³´ì„¸ìš”!
              </p>
            </div>
          )}
          
          {/* ìˆ¨ê²¨ì§„ ì˜¤ë””ì˜¤ ìš”ì†Œ */}
          <audio ref={audioRef} className="hidden" />
        </div>
      </div>

      {/* êµ¬ê°• êµ¬ì¡° í•™ìŠµ ë²„íŠ¼ - completed ìƒíƒœì¼ ë•Œë§Œ í‘œì‹œ */}
      {recordingState === 'completed' && (
        <button
          onClick={handleOralStructureView}
          className="fixed bottom-44 right-6 w-12 h-12 bg-orange-500 text-white rounded-full flex items-center justify-center shadow-lg hover:bg-orange-600 transition-all duration-200 hover:scale-110 z-20"
          title="êµ¬ê°• êµ¬ì¡° í•™ìŠµí•˜ê¸°"
        >
          <Plus size={20} />
        </button>
      )}

      {/* ë…¹ìŒ ì»¨íŠ¸ë¡¤ - ê³ ì • ìœ„ì¹˜ */}
      <div className="fixed bottom-32 left-0 right-0 flex justify-center mb-4">
        <AudioRecorder
          onRecordingComplete={(audioUrl, audioBlob) => {
            console.log('ë…¹ìŒ ì™„ë£Œ:', { audioUrl, audioBlobSize: audioBlob?.size });
            if (audioBlob) {
              setRecordedAudioBlob(audioBlob);
            }
          }}
          autoDownload={false} // ìë™ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™”
          fileName="start-recording.wav"
        >
          {({ isRecording, startRecording, stopRecording }) => (
            <RecordControls
              isRecording={isRecording}
              showHelp={showHelp}
              onToggleRecording={async () => {
                await handleRecordingToggle(!isRecording);
                if (isRecording) {
                  stopRecording();
                } else {
                  // AudioRecorderëŠ” ê¶Œí•œì´ ì´ë¯¸ ìˆì„ ë•Œë§Œ ì‹œì‘
                  if (micPermission === 'granted') {
                    startRecording();
                  }
                }
              }}
              onToggleHelp={toggleHelp}
              disabled={!isSupported || micPermission === 'denied'} // ê¶Œí•œ ê±°ë¶€ ì‹œ ë¹„í™œì„±í™”
            />
          )}
        </AudioRecorder>
      </div>

      {/* í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ë°” - ê³ ì • ìœ„ì¹˜ */}
      <div className="fixed bottom-0 left-0 right-0 w-full">
        <NavBar />
      </div>
    </div>
  );
};

export default StartRecordView;